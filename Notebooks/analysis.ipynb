{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8521eba6",
   "metadata": {},
   "source": [
    "# **Reviews vs Reality.**\n",
    "\n",
    "## **Business Understanding.**\n",
    "### Background.\n",
    "E-commerce is slowly but steadily climbing the ladder to be one of Kenya's economic backbones. With businesses shifting from the traditional trade, that included setting up shops and waiting for customers to walk around looking for what they need, to digital trade where all one needs to do is open an account on any social media platform, post whatever they're selling and just wait for notifications that someone needs their product. No need for physical shop, no pressure, just internet connection and the comfort of their homes.\n",
    "\n",
    "However, such a strong growth in such an industry, comes with really stiff competition. Every seller wants to be the best, to sell the most and earn the most which in result pushes sellers to the extremes of buying fake reviews.\n",
    "\n",
    "### Problem Statement.\n",
    "In Kenya, entreprenuers, both young and old are turning to platforms like Jumia, Killimall, Jiji, PigiaMe among others to sell products and make ends meet. It's not only a business, but a survival.\n",
    "\n",
    "The rise of fake reviews, however, threatens the honest business persons. There have been not only reports but also companies have come up and stated openly that they do, in fact, sell online reviews. This practice not only creates an unfair playing field for vendors because the vendors buying reviews are often better funded than the ones that do not but also denies the struggling vendors a way to earn a living.\n",
    "\n",
    "As a result of fake reviews;\n",
    "- Good products go unseen as the algorithms normally show products with high ratings so genuine sellers who actually rely on real customer feedback get burried in search results. \n",
    "\n",
    "- Honest sellers lose customers as with a low number of reviews, their products are percieved as lower quality.\n",
    "\n",
    "Online platforms need a way to detect mismatches between reviews and ratings, to surface truly trustworthy sellers and protect buyers and sellers alike. This project seeks to help them both understand if the reviews are true or false.\n",
    "\n",
    "### Project Objectives.\n",
    "This project is looking to;\n",
    "- Use Natural Language Processing and sentiment analysis to spot suspicious products whose ratings do not match what people are really saying.\n",
    "\n",
    "### Success criteria.\n",
    "When fake visisbility wins over genuine value, everyone is affected. What would spell success for this project would be that;\n",
    "\n",
    "- Sellers get equal visibility based on real customer feedback.\n",
    "\n",
    "- Customers get protected against cons\n",
    "\n",
    "- Companies get to protect their customers from falling victims of fake reviews \n",
    "\n",
    "### Stakeholders.\n",
    "- Customers: The individuals or businesses purchasing goods or services through the platform.\n",
    "\n",
    "- Sellers/Merchants: Businesses or individuals who list and sell products or services on the platform. \n",
    "\n",
    "- Platform Providers: The company or organization that owns and operates the online commerce platform.\n",
    "\n",
    "- Regulatory Bodies: Government agencies and other organizations that set rules and standards for online commerce, including consumer protection.\n",
    "\n",
    "- Investors: Individuals or organizations that have provided funding for the platform. \n",
    "\n",
    "- Government and International Organizations: These entities are stakeholders due to their role in setting and enforcing regulations related to online commerce. \n",
    "\n",
    "### Project plan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0ad039",
   "metadata": {},
   "source": [
    "## **Data Understanding.**\n",
    "### Data Source.\n",
    "The data used in this project was collected from publicly accessible product pages on `Jumia Kenya` using web scraping techniques. Reviews were gathered from selected products in three categories: fashion, appliances, and other electronics.\n",
    "\n",
    "The scraping was performed using Python libraries such as requests and BeautifulSoup as evident in the `Scraper` folder, and all information collected is visible to any user visiting the site. No login or bypassing of protections was required.\n",
    "\n",
    "This data was collected strictly for educational and research purposes, with the intent of exploring the relationship between product star ratings and actual customer sentiment. It is not affiliated with, endorsed by, or intended to defame Jumia or any of its sellers. It simply aims to uncover insights from publicly available information and promote transparency in digital marketplaces.\n",
    "\n",
    "The dataset is under the file path `Data/`.\n",
    "\n",
    "### Why is the data suitable for this project?\n",
    "Jumia Kenya is one of the most popular e-commerce platforms in Kenya, data from the platform not only provides a larger pool of sellers but also a richer variety of reviews which allows for a more comprehensive understanding of consumer behavior.\n",
    "\n",
    "- The largest e-commerce companies have millions or even billions of transactions, providing a vast dataset to analyze. This allows for more robust statistical analysis and reduces the chance of drawing inaccurate conclusions based on limited data.\n",
    "\n",
    "- Large companies also serve a diverse customer base, including demographics, geographic locations, and purchasing habits. This diverse data helps in identifying patterns that might be missed in smaller datasets focused on specific niches.\n",
    "\n",
    "### Exploring the dataset for understanding.\n",
    "In this section we will be carrying out both qualitative and quantitative analysis to understand the structure of the dataset as well as identify areas that would impact our analysis if left unchecked or simply not fixed.\n",
    "\n",
    "#### Import dependencies and loading the dataset.\n",
    "In this section we will be importing our dependencies that we will be using all through the project for our data cleaning, exploratory data analysis, NPL etc. We will also be loading the scraped dataset.\n",
    "\n",
    "We are loading the dataset using pandas .read_csv() method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67bcaff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies.\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset.\n",
    "reviews = pd.read_csv('../Data/Raw/jumia_reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f2c012",
   "metadata": {},
   "source": [
    "#### Qualitative Analysis.\n",
    "Before our analysis, we will explore our data, to understand its structure and what it contains. We will also be looking at the contenet and making sure it is in good quality for analysis.\n",
    "\n",
    "We will use the .head() method to access the first 5 rows of the data. This will help us understand what columns we have and what type of values they contain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d10cc3f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>category</th>\n",
       "      <th>review_title</th>\n",
       "      <th>review_text</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_date</th>\n",
       "      <th>verified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Berrykey Hawaiian Shirt</td>\n",
       "      <td>fashion</td>\n",
       "      <td>big size not cotton</td>\n",
       "      <td>Not cotton</td>\n",
       "      <td>1</td>\n",
       "      <td>19-06-2025</td>\n",
       "      <td>Verified Purchase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Berrykey Hawaiian Shirt</td>\n",
       "      <td>fashion</td>\n",
       "      <td>Not satisfied</td>\n",
       "      <td>The material is bad.not what I expected</td>\n",
       "      <td>1</td>\n",
       "      <td>13-06-2025</td>\n",
       "      <td>Verified Purchase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Berrykey Hawaiian Shirt</td>\n",
       "      <td>fashion</td>\n",
       "      <td>I like it</td>\n",
       "      <td>It is okay</td>\n",
       "      <td>5</td>\n",
       "      <td>12-05-2025</td>\n",
       "      <td>Verified Purchase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Berrykey Hawaiian Shirt</td>\n",
       "      <td>fashion</td>\n",
       "      <td>I like it</td>\n",
       "      <td>The quality is good. It's worth the price</td>\n",
       "      <td>5</td>\n",
       "      <td>22-04-2025</td>\n",
       "      <td>Verified Purchase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Berrykey Hawaiian Shirt</td>\n",
       "      <td>fashion</td>\n",
       "      <td>good</td>\n",
       "      <td>Good</td>\n",
       "      <td>5</td>\n",
       "      <td>27-01-2025</td>\n",
       "      <td>Verified Purchase</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              product_name category         review_title  \\\n",
       "0  Berrykey Hawaiian Shirt  fashion  big size not cotton   \n",
       "1  Berrykey Hawaiian Shirt  fashion        Not satisfied   \n",
       "2  Berrykey Hawaiian Shirt  fashion            I like it   \n",
       "3  Berrykey Hawaiian Shirt  fashion            I like it   \n",
       "4  Berrykey Hawaiian Shirt  fashion                 good   \n",
       "\n",
       "                                 review_text  rating review_date  \\\n",
       "0                                 Not cotton       1  19-06-2025   \n",
       "1    The material is bad.not what I expected       1  13-06-2025   \n",
       "2                                 It is okay       5  12-05-2025   \n",
       "3  The quality is good. It's worth the price       5  22-04-2025   \n",
       "4                                       Good       5  27-01-2025   \n",
       "\n",
       "            verified  \n",
       "0  Verified Purchase  \n",
       "1  Verified Purchase  \n",
       "2  Verified Purchase  \n",
       "3  Verified Purchase  \n",
       "4  Verified Purchase  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset preview.\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c88d508",
   "metadata": {},
   "source": [
    "From the output above, the data has 7 columns and it contains review data of products. To further explore the dataset's structure, we will use pandas .info() method. This method will give us the concise summary of our data, providing us with infomation on the number of rows and columns, number of non-null values, and the columns' datatype. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ec62fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 362 entries, 0 to 361\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   product_name  362 non-null    object\n",
      " 1   category      362 non-null    object\n",
      " 2   review_title  362 non-null    object\n",
      " 3   review_text   362 non-null    object\n",
      " 4   rating        362 non-null    int64 \n",
      " 5   review_date   362 non-null    object\n",
      " 6   verified      362 non-null    object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 19.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# Get dataset's qualitative summary\n",
    "reviews.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eefd175",
   "metadata": {},
   "source": [
    "The dataset contains data stored in 362 rows in columns named product_name, category, review_title, review_text, rating, review_date and verified. Out of the 7 columns, only 1 is stored as an integer and the rest are stored as objects. Additionally, the data seems to have no null values.\n",
    "\n",
    "Next we are ensuring the data has no null values and also looking if our data has duplicate records. We are using pandas .isnull() and .duplicated() methods. This is a crucial step as duplicates or null values impact the quality, accuracy, and reliability of your data, and lead to inaccurate results in your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0f3e7c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product_name    0\n",
       "category        0\n",
       "review_title    0\n",
       "review_text     0\n",
       "rating          0\n",
       "review_date     0\n",
       "verified        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for null values\n",
    "reviews.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19c32b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reviews dataset contains 1 duplicated record.\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicated records.\n",
    "print(f'The reviews dataset contains {reviews.duplicated().sum()} duplicated record.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259372b6",
   "metadata": {},
   "source": [
    "The datast contains no null values but has 1 duplicated record.\n",
    "\n",
    "#### Quantitative Analysis.\n",
    "Here we will be getting the dataset's qualitative summary using the .describe() method. This method generates descriptive statistics, giving summarized statistical description of your data's measures of central tendecy, mean, mode, median, and percentiles as well as measures of spread, standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36e99321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>362.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.204420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.205795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           rating\n",
       "count  362.000000\n",
       "mean     4.204420\n",
       "std      1.205795\n",
       "min      1.000000\n",
       "25%      4.000000\n",
       "50%      5.000000\n",
       "75%      5.000000\n",
       "max      5.000000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get dataset's statistical summary.\n",
    "reviews.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e278caf7",
   "metadata": {},
   "source": [
    "A lot of ratings in the dataset are positive, the mean of the ratings is 4.20 and the 25th percentile is 4.0.\n",
    "\n",
    "#### Data Quality Issues.\n",
    "Data quality issues, are characterized by inaccuracies, incompleteness, and inconsistencies, these issues, if left unchecked/unsolved, can severely impact our analysis by leading to flawed analysis, poor decision-making, among other problems.\n",
    "\n",
    "Our data understanding section has helped us identfy some of the issues in the dataset including;\n",
    "- Inaccuracies - most of the columns in the dataset are stored as objects when they dates.\n",
    "\n",
    "- Duplicates - we found one duplicated record in the dataset.\n",
    "\n",
    "Addressing these problems is crucial for maintaining data integrity and ensuring reliable results across all business operations. In the next section, we will be cleaning the data and preparing it for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64851198",
   "metadata": {},
   "source": [
    "## **Data Preparation.**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
